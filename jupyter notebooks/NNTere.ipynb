{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import re \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ChatTere.txt\") as file:  \n",
    "    data = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "date_pattern = \"\\[(.*?)\\]\"\n",
    "\n",
    "for message in data:\n",
    "    try:\n",
    "        message_dict = {\n",
    "            'datetime': re.search(date_pattern, message).group()[1:-1],\n",
    "            'user': message.split()[3],\n",
    "            'text': message.split(maxsplit=5)[-1]\n",
    "        }\n",
    "        if \"This message was deleted.\" not in message_dict['text']: # Lo que se borra se olvida\n",
    "            df.append(message_dict)\n",
    "    except:\n",
    "        # Whatsapp a veces no lee muy bien los mensajes con saltos de linea\n",
    "        if message.strip() != \"\":\n",
    "            df[-1]['text'] = f\"{df[-1]['text']}{message}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)\n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], infer_datetime_format=True)\n",
    "\n",
    "def quick_classification(text):\n",
    "    if \"audio omitted\" in text:\n",
    "        return \"Audio\"\n",
    "    if any(file_type in text for file_type in ['image omitted', \"video omitted\"]):\n",
    "        return \"Image/Video\"\n",
    "    if any(file_type in text for file_type in ['GIF omitted', \"sticker omitted\"]):\n",
    "        return \"Sticker/GIF\"\n",
    "    if \"document omitted\" in text: \n",
    "        return \"Document\"\n",
    "    if \"Contact card omitted\" in text: \n",
    "        return \"Contact\"\n",
    "    if set(text.strip().lower()) == {'a', 'j'}:\n",
    "        return \"RISA\" # Nos reimos tanto que toc√≥ incluir una categoria para esto\n",
    "    return \"Text\"\n",
    "\n",
    "df['type'] = df['text'].apply(lambda text: quick_classification(text))\n",
    "\n",
    "df['text'] = df['text'].apply(lambda text: text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tere = df[(df['user']=='Teresa') & (df['type']=='Text')].text.str.cat(sep=' \\n')\n",
    "text_vivi = df[(df['user']=='Viviana') & (df['type']=='Text')].text.str.cat(sep=' \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN helper functions and global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "embedding_dim = 64\n",
    "rnn_units = 512\n",
    "\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def loss(labels, logits):\n",
    "      return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "    \n",
    "\n",
    "def generate_text(model, start_string, num_generate=200):\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return(''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN with texts from Tere "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tere\n",
    "vocab = sorted(set(text_tere))\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in text_tere])\n",
    "\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text_tere)//(seq_length+1)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "vocab_size_tere = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (256, None, 64)           12288     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (256, None, 512)          887808    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (256, None, 512)          1575936   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (256, None, 192)          98496     \n",
      "=================================================================\n",
      "Total params: 2,574,528\n",
      "Trainable params: 2,574,528\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()\n",
    "    \n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_tere = f'training_checkpoints_tere/'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir_tere, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_prefix,\n",
    "        save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First round\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 18s 3s/step - loss: 4.9234\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 3.6466\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 3.3919\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 3.3350\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 3.3079\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 3.2766\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 3.2670\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 3.2535\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 3.2240\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 3.1910\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 3.1322\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 3.0379\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.9113\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.8102\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.7327\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.6707\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.6103\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.5464\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.4893\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.4367\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.3963\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.3644\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.3334\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.3099\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.2884\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.2639\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.2384\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.2244\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.1967\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.1788\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.1602\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.1372\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.1168\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.0997\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.0859\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.0638\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.0417\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.0236\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2.0051\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.9899\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.9689\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.9486\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.9324\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.9183\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.8961\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.8796\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.8578\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.8449\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.8269\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.8131\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.7996\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.7771\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.7622\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.7505\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.7346\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.7205\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.7026\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.6881\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.6766\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.6640\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.6516\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.6344\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.6200\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.6068\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.5958\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.5840\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.5710\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.5582\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.5454\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.5300\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.5195\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.5078\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.5015\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.4875\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.4746\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.4638\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.4480\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.4400\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.4298\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.4167\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.4034\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.3948\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.3828\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.3728\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.3608\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.3516\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.3422\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.3315\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.3192\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.3097\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.2971\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.2878\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.2782\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.2657\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.2560\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.2425\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.2335\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.2251\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.2155\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.2019\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.1922\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.1786\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.1724\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.1582\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1.1483\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.1353\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.1271\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.1155\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.1033\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.0925\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.0802\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.0696\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.0580\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.0525\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.0371\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.0358\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.0200\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 1.0090\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.9962\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.9799\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.9676\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.9588\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.9506\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.9361\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.9206\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.9090\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.8975\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.8854\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.8736\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.8633\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.8577\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.8469\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.8327\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.8169\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.8093\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.7991\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.7900\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.7768\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.7612\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.7496\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.7390\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.7296\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.7161\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.7041\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.6932\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.6833\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.6728\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.6698\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.6551\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.6437\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.6341\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.6198\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.6133\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.6017\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.5906\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.5826\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.5712\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.5661\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.5571\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.5514\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.5379\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.5296\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.5186\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.5168\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.5108\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.4974\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.4870\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.4787\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.4710\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.4653\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.4596\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.4504\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.4456\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.4378\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.4331\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.4334\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.4238\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.4158\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.4095\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.4009\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3979\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3901\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3825\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3809\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3773\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.3718\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.3703\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3648\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.3574\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3571\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3525\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3481\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3444\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.3406\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3381\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3337\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.3309\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3287\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3229\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.3175\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3169\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.3131\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3093\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3063\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.3047\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.3025\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2989\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 15s 3s/step - loss: 0.2973\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.2959\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2924\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.2918\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.2915\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2885\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2823\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2815\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.2816\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.2772\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2790\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2747\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2734\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.2712\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2719\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.2693\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.2668\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2687\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.2634\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2643\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.2607\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 17s 3s/step - loss: 0.2617\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.2594\n",
      "Train for 6 steps\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.2598\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2565\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2575\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2537\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2547\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2514\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2510\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2492\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.2465\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.2483\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    EPOCHS=10\n",
    "    try:\n",
    "        model.load_weights(tf.train.latest_checkpoint(checkpoint_dir_tere))\n",
    "    except:\n",
    "        print(\"First round\")\n",
    "    history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "    if history.history['loss'][-1] < .1:\n",
    "        break\n",
    "    if history.history['loss'][-1] > history.history['loss'][-2]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tere_model = build_model(vocab_size_tere, embedding_dim, rnn_units, batch_size=1)\n",
    "tere_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir_tere))\n",
    "tere_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tu sabes que era mejor \n",
      "Viajar el 14 \n",
      "Yo tambi√©n much√≠simo \n",
      "Estoy toda corta venas \n",
      "Escuchando a \n",
      "James Blarmo mas col una vieja que ten√≠a que pasar \n",
      "Como ü§§.ü§§ \n",
      "O sea mlo \n",
      "Parce \n",
      "Y dela para te ves co\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(tere_model, start_string=u\"Hola amiga \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN with texts from Vivi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vivi\n",
    "vocab = sorted(set(text_vivi))\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in text_vivi])\n",
    "\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text_vivi)//(seq_length+1)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "vocab_size_vivi = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (256, None, 64)           12288     \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (256, None, 512)          887808    \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (256, None, 512)          1575936   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (256, None, 192)          98496     \n",
      "=================================================================\n",
      "Total params: 2,574,528\n",
      "Trainable params: 2,574,528\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()\n",
    "    \n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_vivi = f'training_checkpoints_vivi/'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir_vivi, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_prefix,\n",
    "        save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 7 steps\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 18s 3s/step - loss: 0.2643\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.2660\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 21s 3s/step - loss: 0.2626\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.2595\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 19s 3s/step - loss: 0.2620\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.2575\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.2553\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 19s 3s/step - loss: 0.2589\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 19s 3s/step - loss: 0.2546\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 19s 3s/step - loss: 0.2512\n",
      "Train for 7 steps\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.2524\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 19s 3s/step - loss: 0.2512\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 19s 3s/step - loss: 0.2527\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 19s 3s/step - loss: 0.2483\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 19s 3s/step - loss: 0.2471\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 19s 3s/step - loss: 0.2493\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 19s 3s/step - loss: 0.2488\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 19s 3s/step - loss: 0.2444\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 19s 3s/step - loss: 0.2472\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 19s 3s/step - loss: 0.2431\n",
      "Train for 7 steps\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.2440\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.2447\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.2435\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.2391\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.2413\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.2384\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.2421\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.2386\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 19s 3s/step - loss: 0.2369\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.2381\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    EPOCHS=10\n",
    "    try:\n",
    "        model.load_weights(tf.train.latest_checkpoint(checkpoint_dir_vivi))\n",
    "    except:\n",
    "        print(\"First round\")\n",
    "    history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "    if history.history['loss'][-1] < .1:\n",
    "        break\n",
    "    if history.history['loss'][-1] > history.history['loss'][-2]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vivi_model = build_model(vocab_size_vivi, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "vivi_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir_vivi))\n",
    "\n",
    "vivi_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hacer por tema gayando el audio de mi vida \n",
      "Listo yo le digo \n",
      "Si obvio, si sientes , en mi diario de que es internet y sali√≥ hace TRECE A√ëOS \n",
      "WTF \n",
      "No me siento \n",
      "Estoy re \n",
      "‚ù§Ô∏è \n",
      "üòçü•∞üòçü•∞ \n",
      "LO SEEEEEE üòçü•∞üòçü•∞ \n",
      "Po\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(vivi_model, start_string=u\"Hola amiga \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab_user(user):\n",
    "    global vocab, char2idx, idx2char, text_as_int\n",
    "    if user==\"Tere\":\n",
    "        vocab = sorted(set(text_tere))\n",
    "        char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "        idx2char = np.array(vocab)\n",
    "        text_as_int = np.array([char2idx[c] for c in text_tere])\n",
    "    if user==\"Vivi\":\n",
    "        vocab = sorted(set(text_vivi))\n",
    "        char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "        idx2char = np.array(vocab)\n",
    "        text_as_int = np.array([char2idx[c] for c in text_vivi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = [('Tere', \"Amiga\")]\n",
    "\n",
    "# Initiate convo\n",
    "load_vocab_user(\"Tere\")\n",
    "text_generated = generate_text(tere_model, start_string=convo[-1][1])\n",
    "#text_generated = re.findall('.[^A-Z]*', text_generated)\n",
    "text_generated = text_generated.split(\"\\n\")\n",
    "convo.append((\"Tere\", text_generated[1].strip(\"\\n\")))\n",
    "\n",
    "load_vocab_user(\"Vivi\")\n",
    "text_generated = generate_text(vivi_model, start_string=convo[-1][1])\n",
    "#text_generated = re.findall('.[^A-Z]*', text_generated)\n",
    "text_generated = text_generated.split(\"\\n\")\n",
    "convo.append((\"Vivi\", text_generated[1].strip(\"\\n\")))\n",
    "convo.append((\"Vivi\", text_generated[2].strip(\"\\n\")))\n",
    "\n",
    "# Let the bots have their convo\n",
    "for i in range(0,5):\n",
    "    load_vocab_user(\"Tere\")\n",
    "    text_generated = generate_text(tere_model, start_string=convo[-1][1])\n",
    "    #text_generated = re.findall('.[^A-Z]*', text_generated)\n",
    "    text_generated = text_generated.split(\"\\n\")\n",
    "    for j in range(1,len(text_generated)-1):\n",
    "        convo.append((\"Tere\", text_generated[j].strip(\"\\n\")))\n",
    "    \n",
    "    load_vocab_user(\"Vivi\")\n",
    "    text_generated = generate_text(vivi_model, start_string=convo[-1][1])\n",
    "    #text_generated = re.findall('.[^A-Z]*', text_generated)\n",
    "    text_generated = text_generated.split(\"\\n\")\n",
    "    for j in range(1,len(text_generated)-1):\n",
    "        convo.append((\"Vivi\", text_generated[j].strip(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m\u001b[1mTere            :\u001b[0m Amiga\n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m No entiendo \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m eso que Fer con almuerzo \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Y ah√≠ ch√©ri \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Lo recuerdo a ti \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Ah no amiga \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Ya atenanar todos? \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m üò±üò±üò±üò± \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Muchas gracias por tus audios \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Los valoro un mont√≥n \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Te amo \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Le capido damos \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Una cosa \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Y hubieran despirme que todo es posible \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Cero acostu√≥n? \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Yo tambi√©n \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Me puse a tu √Ångela Jajajaja \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m El primer lagos? \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Que fallarle todas estas historias \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Quiero escuchad \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Amiga te tengo que cancelar pasaje \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Me gusta mucho \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Confignimo \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m mullicado, c√≥mo rojo \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Est√°s como con el ton L terne √©l ya es como tu distoria \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Pero ser maen cuaditos \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Uno necars estoy importaban constan y una tienes m√°s \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m No entender, a quedar \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Que hay despu√©s me direro \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Ma√±ana la botella \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m üíú \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Es demasiado rico \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Amiga ven \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Maldita frialdad tan bien \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m JAJAJAJAJAJAJAJAJAJAJAJAJAJAJAJAJAJAJAJAJAAJAJAJSJS \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m L \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Que no recomiendes estados \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m No es la bien \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Me gusta mucho \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Y qu√© es lo que tu hayas \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Y lo sensidar \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Amiga \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Pero muy tan \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Le toma \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m No sabes que mi mama no se hahaha \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Uy no reconoc√≠ a nientras vidas jajaj \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Jajajaja me encanta el drama \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Tienes fotos de David? \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m LE AMOGO quien te extra√±a \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Me puedo a √©l. No te ha convertido? \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m please! \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Mira esto \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Que fue un puesto dolciendo ‚òπÔ∏è \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Esa ni√±a tiene una en un me su de Marcios por no lo que no \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Por sarte un abrazo \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m ‚òπÔ∏è‚òπÔ∏è‚òπÔ∏è \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Sabes que si nos quiere decir algo Jajajaja \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Lo se \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Estoy demasiado emocionada \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Que belleza \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m A que perdi√≥ todos los puntos \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Que tipo de resprita de vivir con la misma vida del audio \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Mk no \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Pero ese JEMASISO! TODO \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m ESO \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Eso me encanta \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Porque de verdad el man \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Es un piloma \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Es una hp \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m UNA HP COMPLETA \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m üíîüò≠üíîüò≠üíîüò≠üíîüò≠üíîüò≠üò≠üò± \n",
      "\u001b[94m\u001b[1mTere            :\u001b[0m Qu√© guapas \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Esta vez que tambi√©n me dio mama \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Ahora es tu sugar daughter \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Y parece... uno no deja m√°s de 12 horas en visto un mensaje de esos \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Quiz√°s un dolor del agua y el mensaje \n",
      "\u001b[95m\u001b[1mVivi            :\u001b[0m Te mentiste eso Deando \n"
     ]
    }
   ],
   "source": [
    "for message in convo:\n",
    "    if message[0] == \"Tere\":\n",
    "        print(f\"\\033[94m\\033[1m{message[0]: <16}:\\033[0m {message[1]}\")\n",
    "    if message[0] == \"Vivi\":\n",
    "        print(f\"\\033[95m\\033[1m{message[0]: <16}:\\033[0m {message[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
