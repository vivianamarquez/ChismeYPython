{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import re \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ChatTere.txt\") as file:  \n",
    "    data = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "date_pattern = \"\\[(.*?)\\]\"\n",
    "\n",
    "for message in data:\n",
    "    try:\n",
    "        message_dict = {\n",
    "            'datetime': re.search(date_pattern, message).group()[1:-1],\n",
    "            'user': message.split()[3],\n",
    "            'text': message.split(maxsplit=5)[-1]\n",
    "        }\n",
    "        if \"This message was deleted.\" not in message_dict['text']: # Lo que se borra se olvida\n",
    "            df.append(message_dict)\n",
    "    except:\n",
    "        # Whatsapp a veces no lee muy bien los mensajes con saltos de linea\n",
    "        if message.strip() != \"\":\n",
    "            df[-1]['text'] = f\"{df[-1]['text']}{message}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)\n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], infer_datetime_format=True)\n",
    "\n",
    "def quick_classification(text):\n",
    "    if \"audio omitted\" in text:\n",
    "        return \"Audio\"\n",
    "    if any(file_type in text for file_type in ['image omitted', \"video omitted\"]):\n",
    "        return \"Image/Video\"\n",
    "    if any(file_type in text for file_type in ['GIF omitted', \"sticker omitted\"]):\n",
    "        return \"Sticker/GIF\"\n",
    "    if \"document omitted\" in text: \n",
    "        return \"Document\"\n",
    "    if \"Contact card omitted\" in text: \n",
    "        return \"Contact\"\n",
    "    if set(text.strip().lower()) == {'a', 'j'}:\n",
    "        return \"RISA\" # Nos reimos tanto que toc√≥ incluir una categoria para esto\n",
    "    return \"Text\"\n",
    "\n",
    "df['type'] = df['text'].apply(lambda text: quick_classification(text))\n",
    "\n",
    "df['text'] = df['text'].apply(lambda text: text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\n",
    "for user,text in zip(df.user.values, df.text.values):\n",
    "    data = data + f\"{user}:\\t\\t{text}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "embedding_dim = 64\n",
    "rnn_units = 512\n",
    "\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def loss(labels, logits):\n",
    "      return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "    \n",
    "\n",
    "def generate_text(model, start_string, num_generate=200):\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return(''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(data))\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in data])\n",
    "\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(data)//(seq_length+1)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (256, None, 64)           16384     \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (256, None, 512)          887808    \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (256, None, 512)          1575936   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (256, None, 256)          131328    \n",
      "=================================================================\n",
      "Total params: 2,611,456\n",
      "Trainable params: 2,611,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()\n",
    "    \n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_mixed = f'training_checkpoints_mixed/'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir_mixed, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_prefix,\n",
    "        save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 1.0511\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 70s 3s/step - loss: 1.0364\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 69s 3s/step - loss: 1.0245\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 67s 3s/step - loss: 1.0128\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 69s 3s/step - loss: 1.0007\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 72s 3s/step - loss: 0.9895\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 67s 3s/step - loss: 0.9794\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 66s 3s/step - loss: 0.9701\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.9592\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.9509\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.9406\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.9312\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 69s 3s/step - loss: 0.9218\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 71s 3s/step - loss: 0.9133\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 68s 3s/step - loss: 0.9045\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 69s 3s/step - loss: 0.8944\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 69s 3s/step - loss: 0.8871\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.8787\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 68s 3s/step - loss: 0.8690\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.8599\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 68s 3s/step - loss: 0.8517\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 71s 3s/step - loss: 0.8436\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 72s 3s/step - loss: 0.8347\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 69s 3s/step - loss: 0.8266\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 68s 3s/step - loss: 0.8188\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 68s 3s/step - loss: 0.8099\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 68s 3s/step - loss: 0.8021\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 68s 3s/step - loss: 0.7934\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 66s 3s/step - loss: 0.7857\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.7776\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.7669\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 67s 3s/step - loss: 0.7597\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.7524\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.7438\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.7350\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.7266\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.7181\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.7102\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.7018\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.6909\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.6837\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.6775\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.6672\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.6591\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 67s 3s/step - loss: 0.6520\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.6422\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.6348\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.6286\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.6197\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.6104\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 68s 3s/step - loss: 0.6036\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.5967\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 66s 3s/step - loss: 0.5896\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 67s 3s/step - loss: 0.5823\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.5749\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.5685\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.5609\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.5527\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 67s 3s/step - loss: 0.5481\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 69s 3s/step - loss: 0.5411\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 69s 3s/step - loss: 0.5346\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 66s 3s/step - loss: 0.5263\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.5190\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.5145\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.5069\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.5012\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.4982\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.4908\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.4842\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 68s 3s/step - loss: 0.4763\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 67s 3s/step - loss: 0.4729\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 68s 3s/step - loss: 0.4660\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.4631\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.4596\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.4551\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 67s 3s/step - loss: 0.4487\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.4444\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.4396\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 69s 3s/step - loss: 0.4333\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 67s 3s/step - loss: 0.4300\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.4240\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 67s 3s/step - loss: 0.4221\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.4155\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 66s 3s/step - loss: 0.4101\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.4070\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.4028\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.4008\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.3969\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.3927\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.3883\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.3866\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.3833\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.3788\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.3744\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.3712\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.3717\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.3643\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.3601\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.3588\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.3558\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 61s 3s/step - loss: 0.3523\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.3489\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.3464\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.3442\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.3414\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.3391\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.3365\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.3346\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.3320\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.3290\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.3257\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.3231\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 70s 3s/step - loss: 0.3240\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 73s 3s/step - loss: 0.3191\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 72s 3s/step - loss: 0.3151\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.3139\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 66s 3s/step - loss: 0.3105\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 72s 3s/step - loss: 0.3096\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 69s 3s/step - loss: 0.3071\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.3053\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.3046\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.3045\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 70s 3s/step - loss: 0.3012\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 70s 3s/step - loss: 0.2968\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 68s 3s/step - loss: 0.2941\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 72s 3s/step - loss: 0.2914\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.2909\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.2910\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.2873\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.2850\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.2827\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.2820\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.2798\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.2790\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.2763\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.2769\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.2761\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 66s 3s/step - loss: 0.2748\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.2731\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 67s 3s/step - loss: 0.2722\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.2713\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.2683\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.2689\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.2664\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 68s 3s/step - loss: 0.2636\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.2629\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 70s 3s/step - loss: 0.2612\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 73s 3s/step - loss: 0.2601\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 72s 3s/step - loss: 0.2583\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 68s 3s/step - loss: 0.2567\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 73s 3s/step - loss: 0.2566\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 70s 3s/step - loss: 0.2544\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 71s 3s/step - loss: 0.2549\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 75s 3s/step - loss: 0.2533\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 70s 3s/step - loss: 0.2521\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 71s 3s/step - loss: 0.2528\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 73s 3s/step - loss: 0.2506\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 72s 3s/step - loss: 0.2519\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.2491\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.2485\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.2473\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.2460\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.2451\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.2447\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.2438\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.2430\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.2425\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 66s 3s/step - loss: 0.2418\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.2413\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 73s 3s/step - loss: 0.2413\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 74s 3s/step - loss: 0.2404\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 72s 3s/step - loss: 0.2377\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 71s 3s/step - loss: 0.2378\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 71s 3s/step - loss: 0.2359\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 71s 3s/step - loss: 0.2364\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 78s 4s/step - loss: 0.2355\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 74s 3s/step - loss: 0.2350\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 79s 4s/step - loss: 0.2345\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 67s 3s/step - loss: 0.2345\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 68s 3s/step - loss: 0.2331\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 67s 3s/step - loss: 0.2336\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.2313\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 67s 3s/step - loss: 0.2313\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 66s 3s/step - loss: 0.2303\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 66s 3s/step - loss: 0.2307\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 66s 3s/step - loss: 0.2297\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 65s 3s/step - loss: 0.2280\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 73s 3s/step - loss: 0.2271\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.2283\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.2260\n",
      "Train for 22 steps\n",
      "Epoch 1/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.2253\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.2242\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.2245\n",
      "Epoch 4/10\n",
      " 5/22 [=====>........................] - ETA: 57s - loss: 0.2195"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c04be2cd720b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"First round\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    EPOCHS=10\n",
    "    try:\n",
    "        model.load_weights(tf.train.latest_checkpoint(checkpoint_dir_mixed))\n",
    "    except:\n",
    "        print(\"First round\")\n",
    "    history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "    if history.history['loss'][-1] < .1:\n",
    "        break\n",
    "    if history.history['loss'][-1] > history.history['loss'][-2]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir_mixed))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teresa:\t\tY yo te pod√≠a lo d√≠a a decir que te extra√±a\n",
      "Viviana:\t\t‚Äéimage omitted\n",
      "Viviana:\t\tX2MINTAS POR TARO POR DIOS R√ÅPIDO QUE NO SAB√çAS DE AMONONO esa vez jajaja\n",
      "Teresa:\t\t‚Äévideo omitted\n",
      "Viviana:\t\tPara de enterar\n",
      "Teresa:\t\tPara veresa:\t\tü§¶üèΩ‚Äç‚ôÄü§¶üèΩ‚Äç‚ôÄ\n",
      "Teresa:\t\tYa estoy siendo se ven idea\n",
      "Viviana:\t\tGracias por tu caratos\n",
      "Teresa:\t\tSIIIIIII\n",
      "Teresa:\t\tJAJAJAJAJAJAJA\n",
      "Viviana:\t\tBueno amiga te voy a enviar las √∫nicas que todo, desde el principio y te puedo llamado\n",
      "Teresa:\t\tNo\n",
      "Viviana:\t\tEste ver. Y autismo\n",
      "Vivia\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Teresa:\\t\\tHola amiga\\n\", num_generate=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
